import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_predict
from sklearn.metrics import (confusion_matrix, accuracy_score, precision_score,
                             recall_score, roc_auc_score, fbeta_score, make_scorer,
                             brier_score_loss)

# ----------------------------
# 1. Data Loading & Preparation
# ----------------------------
# Replace with your actual data loading code. Ensure your CSV contains an "accident" column.
df = pd.read_csv("your_data.csv")
X = df.drop(columns=["accident"])
y = df["accident"]

# ----------------------------
# 2. Hyperparameter Tuning with Cost-Sensitive Learning
# ----------------------------
# Define a parameter grid that includes different regularization strengths and class weights.
param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],
    'class_weight': [None, 'balanced', {0: 1, 1: 2}, {0: 1, 1: 3}]
}

# We'll use an F₂ score (beta=2) to give more weight to recall.
f2_scorer = make_scorer(fbeta_score, beta=2)

# Set up cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Use Logistic Regression with a solver that supports small datasets well.
lr = LogisticRegression(max_iter=1000, solver='liblinear')

# GridSearchCV to tune hyperparameters based on F₂ score.
grid_search = GridSearchCV(lr, param_grid, scoring=f2_scorer, cv=cv, n_jobs=-1)
grid_search.fit(X, y)

print("Best hyperparameters:", grid_search.best_params_)
best_lr = grid_search.best_estimator_

# ----------------------------
# 3. Cross-Validated Probability Predictions
# ----------------------------
# Obtain out-of-sample probability predictions.
y_prob_cv = cross_val_predict(best_lr, X, y, cv=cv, method='predict_proba')[:, 1]

# ----------------------------
# 4. Fine-Tune the Decision Threshold
# ----------------------------
# Define a function to compute metrics at a given threshold.
def evaluate_threshold(threshold, y_true, y_prob):
    y_pred = (y_prob >= threshold).astype(int)
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    acc = accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred, zero_division=0)
    rec = recall_score(y_true, y_pred)
    f2 = fbeta_score(y_true, y_pred, beta=2)
    roc_auc = roc_auc_score(y_true, y_prob)  # Threshold-independent
    brier = brier_score_loss(y_true, y_prob)
    return {
        'threshold': threshold,
        'TN': tn, 'FP': fp, 'FN': fn, 'TP': tp,
        'accuracy': acc,
        'precision': prec,
        'recall': rec,
        'F2': f2,
        'roc_auc': roc_auc,
        'brier': brier
    }

# Sweep over thresholds from 0 to 1.
thresholds = np.linspace(0, 1, 101)
metrics_list = [evaluate_threshold(th, y, y_prob_cv) for th in thresholds]
metrics_df = pd.DataFrame(metrics_list)

# Plot Recall, Precision, and F2 Score versus Threshold.
plt.figure(figsize=(10,6))
plt.plot(metrics_df['threshold'], metrics_df['recall'], label='Recall', marker='o')
plt.plot(metrics_df['threshold'], metrics_df['precision'], label='Precision', marker='o')
plt.plot(metrics_df['threshold'], metrics_df['F2'], label='F2 Score', marker='o')
plt.xlabel('Decision Threshold')
plt.ylabel('Metric Value')
plt.title('Metrics vs. Decision Threshold')
plt.legend()
plt.grid(True)
plt.show()

# Select the threshold that maximizes the F2 score.
best_row = metrics_df.loc[metrics_df['F2'].idxmax()]
best_threshold = best_row['threshold']
print("Best threshold based on F2 score:", best_threshold)
print("Metrics at best threshold:")
print(best_row)

# ----------------------------
# 5. Print Final Labeled Confusion Matrix
# ----------------------------
# Apply the chosen threshold to generate final predictions.
y_pred_final = (y_prob_cv >= best_threshold).astype(int)
cm = confusion_matrix(y, y_pred_final)
cm_df = pd.DataFrame(cm, 
                     index=["Actual No Accident", "Actual Accident"],
                     columns=["Predicted No Accident", "Predicted Accident"])

print("Final Labeled Confusion Matrix:")
print(cm_df)

# Optionally, plot the heatmap.
plt.figure(figsize=(8, 6))
sns.heatmap(cm_df, annot=True, fmt="d", cmap="Blues")
plt.title("Final Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()
