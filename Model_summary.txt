from sklearn.linear_model import LinearRegression
import numpy as np
import pandas as pd
from scipy import stats

def get_sklearn_regression_stats(model, X, y):
    """
    Get regression statistics for sklearn's LinearRegression model
    
    Parameters:
    model: fitted sklearn LinearRegression model
    X: feature matrix
    y: target variable
    """
    # Get predictions and residuals
    y_pred = model.predict(X)
    residuals = y - y_pred
    
    # Calculate statistics
    n = len(y)  # number of observations
    p = X.shape[1]  # number of features
    dof = n - p - 1  # degrees of freedom
    
    # Calculate MSE
    mse = np.sum(residuals**2) / dof
    
    # Calculate standard errors
    X_with_const = np.column_stack([np.ones(len(X)), X])
    var_covar_matrix = mse * np.linalg.inv(X_with_const.T @ X_with_const)
    std_errors = np.sqrt(np.diag(var_covar_matrix))[1:]  # Remove intercept std error
    
    # Calculate t-values and p-values
    t_stats = model.coef_ / std_errors
    p_values = 2 * (1 - stats.t.cdf(abs(t_stats), dof))
    
    # Create DataFrame with results
    results_df = pd.DataFrame({
        'Feature': X.columns if hasattr(X, 'columns') else [f'X{i}' for i in range(X.shape[1])],
        'Coefficient': model.coef_,
        'Std Error': std_errors,
        't-value': t_stats,
        'p-value': p_values
    })
    
    # Add significance stars
    results_df['Significance'] = ''
    results_df.loc[results_df['p-value'] < 0.001, 'Significance'] = '***'
    results_df.loc[(results_df['p-value'] >= 0.001) & (results_df['p-value'] < 0.01), 'Significance'] = '**'
    results_df.loc[(results_df['p-value'] >= 0.01) & (results_df['p-value'] < 0.05), 'Significance'] = '*'
    
    # Add intercept row if it exists
    if hasattr(model, 'intercept_') and model.intercept_ != 0:
        intercept_se = np.sqrt(var_covar_matrix[0,0])
        intercept_t = model.intercept_ / intercept_se
        intercept_p = 2 * (1 - stats.t.cdf(abs(intercept_t), dof))
        
        intercept_row = pd.DataFrame({
            'Feature': ['Intercept'],
            'Coefficient': [model.intercept_],
            'Std Error': [intercept_se],
            't-value': [intercept_t],
            'p-value': [intercept_p],
            'Significance': ['***' if intercept_p < 0.001 else 
                           '**' if intercept_p < 0.01 else 
                           '*' if intercept_p < 0.05 else '']
        })
        results_df = pd.concat([intercept_row, results_df], ignore_index=True)
    
    # Round numeric columns
    numeric_cols = ['Coefficient', 'Std Error', 't-value', 'p-value']
    results_df[numeric_cols] = results_df[numeric_cols].round(4)
    
    # Print additional statistics
    r2 = model.score(X, y)
    adj_r2 = 1 - (1-r2)*(n-1)/(n-p-1)
    
    print(f"\nModel Summary:")
    print(f"R-squared: {r2:.4f}")
    print(f"Adjusted R-squared: {adj_r2:.4f}")
    print(f"Number of observations: {n}")
    print("\nSignificance codes: 0 '***' 0.001 '**' 0.01 '*' 0.05\n")
    
    return results_df

# Example usage:
"""
# Fit your sklearn model
model = LinearRegression()
model.fit(X, y)

# Get statistics
stats_table = get_sklearn_regression_stats(model, X, y)
print(stats_table)
"""










import pandas as pd
import numpy as np
from lightgbm import LGBMRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import matplotlib.pyplot as plt
import seaborn as sns

def create_lgbm_summary(model, X_train, y_train, X_test=None, y_test=None):
    """
    Create a comprehensive summary of LGBMRegressor model performance.
    
    Parameters:
    model: Fitted LGBMRegressor model
    X_train: Training features
    y_train: Training target
    X_test: Test features (optional)
    y_test: Test target (optional)
    
    Returns:
    dict: Dictionary containing all summary metrics and DataFrames
    """
    summary = {}
    
    # Convert X to DataFrame if it's not already
    X_train = pd.DataFrame(X_train) if not isinstance(X_train, pd.DataFrame) else X_train
    
    # Feature Importance
    feature_importance = pd.DataFrame({
        'Feature': X_train.columns,
        'Importance': model.feature_importances_
    })
    feature_importance = feature_importance.sort_values('Importance', ascending=False)
    summary['feature_importance'] = feature_importance
    
    # Training Metrics
    y_train_pred = model.predict(X_train)
    train_metrics = {
        'R-squared (Train)': r2_score(y_train, y_train_pred),
        'MSE (Train)': mean_squared_error(y_train, y_train_pred),
        'RMSE (Train)': np.sqrt(mean_squared_error(y_train, y_train_pred)),
        'MAE (Train)': mean_absolute_error(y_train, y_train_pred)
    }
    
    # Test Metrics (if test data provided)
    if X_test is not None and y_test is not None:
        y_test_pred = model.predict(X_test)
        test_metrics = {
            'R-squared (Test)': r2_score(y_test, y_test_pred),
            'MSE (Test)': mean_squared_error(y_test, y_test_pred),
            'RMSE (Test)': np.sqrt(mean_squared_error(y_test, y_test_pred)),
            'MAE (Test)': mean_absolute_error(y_test, y_test_pred)
        }
        summary['test_metrics'] = test_metrics
    
    summary['train_metrics'] = train_metrics
    
    # Model Parameters
    summary['model_params'] = model.get_params()
    
    # Print Summary
    print("=== LGBMRegressor Model Summary ===\n")
    
    print("Model Performance Metrics:")
    print("-" * 50)
    for metric, value in train_metrics.items():
        print(f"{metric}: {value:.4f}")
    
    if X_test is not None and y_test is not None:
        print("\nTest Set Metrics:")
        print("-" * 50)
        for metric, value in test_metrics.items():
            print(f"{metric}: {value:.4f}")
    
    print("\nTop 10 Feature Importance:")
    print("-" * 50)
    print(feature_importance.head(10).to_string(index=False))
    
    print("\nKey Model Parameters:")
    print("-" * 50)
    important_params = ['n_estimators', 'learning_rate', 'max_depth', 'num_leaves']
    for param in important_params:
        print(f"{param}: {model.get_params()[param]}")
    
    # Create visualizations
    plt.figure(figsize=(12, 6))
    
    # Feature Importance Plot
    plt.subplot(1, 2, 1)
    top_features = feature_importance.head(10)
    sns.barplot(x='Importance', y='Feature', data=top_features)
    plt.title('Top 10 Feature Importance')
    plt.tight_layout()
    
    # Actual vs Predicted Plot
    plt.subplot(1, 2, 2)
    plt.scatter(y_train, y_train_pred, alpha=0.5)
    plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)
    plt.xlabel('Actual Values')
    plt.ylabel('Predicted Values')
    plt.title('Actual vs Predicted (Training Set)')
    plt.tight_layout()
    
    return summary

# Example usage:
if __name__ == "__main__":
    from sklearn.datasets import make_regression
    from sklearn.model_selection import train_test_split
    
    # Create sample data
    X, y = make_regression(n_samples=1000, n_features=10, random_state=42)
    X = pd.DataFrame(X, columns=[f'Feature_{i}' for i in range(X.shape[1])])
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Train model
    model = LGBMRegressor(random_state=42)
    model.fit(X_train, y_train)
    
    # Create summary
    summary = create_lgbm_summary(model, X_train, y_train, X_test, y_test)