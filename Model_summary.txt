import pandas as pd
import numpy as np
from scipy import stats
from sklearn.linear_model import LinearRegression

def create_regression_stats_table(X, y, model=None):
    """
    Create a statistics table for Linear Regression including coefficients,
    standard errors, t-values, and p-values.
    
    Parameters:
    X (array-like): Features/independent variables
    y (array-like): Target/dependent variable
    model (LinearRegression, optional): Pre-fitted model. If None, a new model will be fitted.
    
    Returns:
    pandas.DataFrame: Statistics table
    """
    # Convert X to DataFrame if it's not already
    X = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X
    
    # Fit the model if not provided
    if model is None:
        model = LinearRegression()
        model.fit(X, y)
    
    # Get coefficients
    coefficients = model.coef_
    if hasattr(X, 'columns'):
        feature_names = list(X.columns)
    else:
        feature_names = [f'X{i}' for i in range(X.shape[1])]
    
    # Calculate predictions and residuals
    y_pred = model.predict(X)
    residuals = y - y_pred
    
    # Calculate degrees of freedom
    n = len(y)
    p = len(feature_names)
    dof = n - p - 1
    
    # Calculate MSE and standard errors
    mse = np.sum(residuals ** 2) / dof
    X_matrix = np.matrix(X)
    var_covar_matrix = mse * np.linalg.inv(X_matrix.T @ X_matrix)
    std_errors = np.sqrt(np.diag(var_covar_matrix))
    
    # Calculate t-values and p-values
    t_values = coefficients / std_errors
    p_values = 2 * (1 - stats.t.cdf(abs(t_values), dof))
    
    # Create results DataFrame
    results_df = pd.DataFrame({
        'Feature': feature_names,
        'Coefficient': coefficients,
        'Std Error': std_errors,
        't-value': t_values,
        'p-value': p_values
    })
    
    # Add significance stars
    results_df['Significance'] = ''
    results_df.loc[results_df['p-value'] < 0.001, 'Significance'] = '***'
    results_df.loc[(results_df['p-value'] >= 0.001) & (results_df['p-value'] < 0.01), 'Significance'] = '**'
    results_df.loc[(results_df['p-value'] >= 0.01) & (results_df['p-value'] < 0.05), 'Significance'] = '*'
    
    # Format numeric columns
    results_df['Coefficient'] = results_df['Coefficient'].round(4)
    results_df['Std Error'] = results_df['Std Error'].round(4)
    results_df['t-value'] = results_df['t-value'].round(4)
    results_df['p-value'] = results_df['p-value'].round(4)
    
    # Add intercept information if available
    if hasattr(model, 'intercept_') and model.intercept_ != 0:
        intercept_se = np.sqrt(mse * np.linalg.inv(X_matrix.T @ X_matrix)[0, 0])
        intercept_t = model.intercept_ / intercept_se
        intercept_p = 2 * (1 - stats.t.cdf(abs(intercept_t), dof))
        
        intercept_row = pd.DataFrame({
            'Feature': ['Intercept'],
            'Coefficient': [model.intercept_],
            'Std Error': [intercept_se],
            't-value': [intercept_t],
            'p-value': [intercept_p],
            'Significance': ['***' if intercept_p < 0.001 else 
                           '**' if intercept_p < 0.01 else 
                           '*' if intercept_p < 0.05 else '']
        })
        
        results_df = pd.concat([intercept_row, results_df], ignore_index=True)
    
    # Add R-squared and Adjusted R-squared
    r_squared = model.score(X, y)
    adj_r_squared = 1 - (1 - r_squared) * (n - 1) / (n - p - 1)
    
    print(f"\nModel Summary:")
    print(f"R-squared: {r_squared:.4f}")
    print(f"Adjusted R-squared: {adj_r_squared:.4f}")
    print(f"Number of observations: {n}")
    print("\nSignificance codes: 0 '***' 0.001 '**' 0.01 '*' 0.05\n")
    
    return results_df

# Example usage:
if __name__ == "__main__":
    from sklearn.datasets import make_regression
    
    # Create sample data
    X, y = make_regression(n_samples=100, n_features=3, random_state=42)
    X = pd.DataFrame(X, columns=['Feature1', 'Feature2', 'Feature3'])
    
    # Get statistics table
    stats_table = create_regression_stats_table(X, y)
    print("\nRegression Statistics:")
    print(stats_table)











import pandas as pd
import numpy as np
from lightgbm import LGBMRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import matplotlib.pyplot as plt
import seaborn as sns

def create_lgbm_summary(model, X_train, y_train, X_test=None, y_test=None):
    """
    Create a comprehensive summary of LGBMRegressor model performance.
    
    Parameters:
    model: Fitted LGBMRegressor model
    X_train: Training features
    y_train: Training target
    X_test: Test features (optional)
    y_test: Test target (optional)
    
    Returns:
    dict: Dictionary containing all summary metrics and DataFrames
    """
    summary = {}
    
    # Convert X to DataFrame if it's not already
    X_train = pd.DataFrame(X_train) if not isinstance(X_train, pd.DataFrame) else X_train
    
    # Feature Importance
    feature_importance = pd.DataFrame({
        'Feature': X_train.columns,
        'Importance': model.feature_importances_
    })
    feature_importance = feature_importance.sort_values('Importance', ascending=False)
    summary['feature_importance'] = feature_importance
    
    # Training Metrics
    y_train_pred = model.predict(X_train)
    train_metrics = {
        'R-squared (Train)': r2_score(y_train, y_train_pred),
        'MSE (Train)': mean_squared_error(y_train, y_train_pred),
        'RMSE (Train)': np.sqrt(mean_squared_error(y_train, y_train_pred)),
        'MAE (Train)': mean_absolute_error(y_train, y_train_pred)
    }
    
    # Test Metrics (if test data provided)
    if X_test is not None and y_test is not None:
        y_test_pred = model.predict(X_test)
        test_metrics = {
            'R-squared (Test)': r2_score(y_test, y_test_pred),
            'MSE (Test)': mean_squared_error(y_test, y_test_pred),
            'RMSE (Test)': np.sqrt(mean_squared_error(y_test, y_test_pred)),
            'MAE (Test)': mean_absolute_error(y_test, y_test_pred)
        }
        summary['test_metrics'] = test_metrics
    
    summary['train_metrics'] = train_metrics
    
    # Model Parameters
    summary['model_params'] = model.get_params()
    
    # Print Summary
    print("=== LGBMRegressor Model Summary ===\n")
    
    print("Model Performance Metrics:")
    print("-" * 50)
    for metric, value in train_metrics.items():
        print(f"{metric}: {value:.4f}")
    
    if X_test is not None and y_test is not None:
        print("\nTest Set Metrics:")
        print("-" * 50)
        for metric, value in test_metrics.items():
            print(f"{metric}: {value:.4f}")
    
    print("\nTop 10 Feature Importance:")
    print("-" * 50)
    print(feature_importance.head(10).to_string(index=False))
    
    print("\nKey Model Parameters:")
    print("-" * 50)
    important_params = ['n_estimators', 'learning_rate', 'max_depth', 'num_leaves']
    for param in important_params:
        print(f"{param}: {model.get_params()[param]}")
    
    # Create visualizations
    plt.figure(figsize=(12, 6))
    
    # Feature Importance Plot
    plt.subplot(1, 2, 1)
    top_features = feature_importance.head(10)
    sns.barplot(x='Importance', y='Feature', data=top_features)
    plt.title('Top 10 Feature Importance')
    plt.tight_layout()
    
    # Actual vs Predicted Plot
    plt.subplot(1, 2, 2)
    plt.scatter(y_train, y_train_pred, alpha=0.5)
    plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)
    plt.xlabel('Actual Values')
    plt.ylabel('Predicted Values')
    plt.title('Actual vs Predicted (Training Set)')
    plt.tight_layout()
    
    return summary

# Example usage:
if __name__ == "__main__":
    from sklearn.datasets import make_regression
    from sklearn.model_selection import train_test_split
    
    # Create sample data
    X, y = make_regression(n_samples=1000, n_features=10, random_state=42)
    X = pd.DataFrame(X, columns=[f'Feature_{i}' for i in range(X.shape[1])])
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Train model
    model = LGBMRegressor(random_state=42)
    model.fit(X_train, y_train)
    
    # Create summary
    summary = create_lgbm_summary(model, X_train, y_train, X_test, y_test)